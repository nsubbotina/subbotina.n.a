{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 5877)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=ENGLISH_STOP_WORDS,\n",
    "                             analyzer='word', binary=True, min_df=20, max_df=.04)\n",
    "vectorizer.fit(newsgroups_train.data)\n",
    "\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [33:36<00:00, 39.39s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def LDA (X, topics, a, b, n_iter ):\n",
    "    n_kw = np.zeros((topics, X.shape[1]))\n",
    "    n_dk = np.zeros((X.shape[0], topics))\n",
    "    n_k = np.zeros(topics)\n",
    "\n",
    "    docs, words = X.nonzero()\n",
    "    z = np.random.choice(topics, len(docs))\n",
    "    \n",
    "    for word, doc, z_ in zip(words, docs, z):\n",
    "        n_kw[z_,word] += 1\n",
    "        n_dk[doc,z_] += 1\n",
    "        n_k[z_] +=1\n",
    "  \n",
    "    for i in tqdm(range(n_iter)):\n",
    "        for j in range(len(docs)):\n",
    "            n_kw[z[j]][words[j]]-=1\n",
    "            n_dk[docs[j]][z[j]]-=1\n",
    "            n_k[z[j]]-=1\n",
    "            \n",
    "            p = (n_dk[docs[j], :] + a)*(n_kw[:, words[j]] + b[words[j]])/(n_k + b.sum())\n",
    "            p /= p.sum()\n",
    "            z[j] = np.random.choice(np.arange(topics), p = p)\n",
    "            \n",
    "            n_kw[z[j]][words[j]]+=1\n",
    "            n_dk[docs[j]][z[j]]+=1\n",
    "            n_k[z[j]]+=1\n",
    "            \n",
    "    return n_kw, n_dk, n_k, z\n",
    "\n",
    "\n",
    "topics = 20\n",
    "n_kw, n_dk, n_k, z = LDA( X_train,topics,  1*np.ones(topics), 1*np.ones(X_train.shape[1]), 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top-10 words in 20 topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\tcontrol\tcrime\tgun\tguns\tlaw\tlaws\tpolice\trights\tself\tweapons\n",
      "Topic 1:\t1993\tapril\tcontact\tmajor\tnational\tpress\tresearch\tstates\tuniversity\twashington\n",
      "Topic 2:\tanybody\tcouldn\tdeleted\tguess\tnice\toh\tsays\tsorry\tsounds\tstuff\n",
      "Topic 3:\tcard\tcomputer\tdisk\tdos\tmac\tmemory\tmonitor\tpc\tspeed\tvideo\n",
      "Topic 4:\tcountry\tisrael\tisraeli\tjewish\tjews\tkilled\tland\tpeace\ttoday\twar\n",
      "Topic 5:\tbible\tchrist\tchristian\tchristians\tdeath\tjesus\tjohn\tlove\tman\tsays\n",
      "Topic 6:\tgame\tgames\thockey\tleague\tplay\tplayers\tseason\tteam\tteams\twin\n",
      "Topic 7:\t11\t12\t13\t14\t16\t17\t18\t23\t24\t25\n",
      "Topic 8:\t100\tasking\tbuy\tcondition\toffer\toriginal\tprice\tsale\tsell\tshipping\n",
      "Topic 9:\tcause\tcommon\tdisease\teffect\texperience\tlarge\tresults\tsimilar\tsmall\tusually\n",
      "Topic 10:\tcame\tdays\thappened\thome\tleft\tsaw\tstarted\ttold\ttook\twent\n",
      "Topic 11:\tarticle\tchange\tcomes\tcouple\tdoubt\thear\tok\tsimply\tunless\twonder\n",
      "Topic 12:\tcheers\tcomes\tcouple\texactly\tgoes\tguess\thaven\tnet\tsort\twouldn\n",
      "Topic 13:\tapplication\tcode\tfile\tfiles\tftp\trunning\tserver\tuser\tversion\twindow\n",
      "Topic 14:\talgorithm\tchip\tclipper\tencryption\tkey\tkeys\tphone\tpublic\tsecure\tsecurity\n",
      "Topic 15:\taddress\tarticle\tcheck\tdate\tmessage\tnet\tnews\tposted\tposting\tsubject\n",
      "Topic 16:\tbike\tcar\tcars\tengine\tfast\tleft\tmiles\troad\tspeed\tturn\n",
      "Topic 17:\tcenter\tcost\tearth\tlaunch\tlow\tmoon\tnasa\torbit\tscience\tspace\n",
      "Topic 18:\tagree\targument\tclaim\tevidence\texist\tmatter\tmind\treligion\tsaying\tword\n",
      "Topic 19:\tadvance\tappreciate\tappreciated\tcurrent\thi\tinfo\tinput\tlow\tsimple\tthank\n"
     ]
    }
   ],
   "source": [
    "top_10 = np.argsort(n_kw, axis=1)[:, :-11:-1]\n",
    "\n",
    "for t in range(20):\n",
    "    doc = np.zeros((1, X_train.shape[1]))\n",
    "    for word in top_10[t]:\n",
    "        doc[0, word] = 1\n",
    "    print('Topic {}:\\t{}'.format(t, '\\t'.join(vectorizer.inverse_transform(doc)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно соотнести полученные топики с топиками из датасета. Так например, 8 полученный топик явно относится к хоккею, 17 топик возможно к космосу, 3, 13 и 14 к электронике и ОС, 16 к машинам, 5 к религии"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
